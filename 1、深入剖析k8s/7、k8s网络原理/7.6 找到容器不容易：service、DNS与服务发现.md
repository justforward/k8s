
k8s中service究竟是如何工作的？

service是由kube-proxy组件加上iptables共同实现的。

或者通过ipvs的方式。如何查看[k8s](https://so.csdn.net/so/search?q=k8s&spm=1001.2101.3001.7020)集群中kube-proxy的代理模式呢？

```shell
# 1、访问kube-proxy的接口
ginkgo :: ~/config/service % curl localhost:10249/proxyMode
ipvs%  # 该集群的代理模式是ipvs

```



1、通过iptables的方式

在创建一个名字为hostname的service来说，一旦被提交给k8s，那么kube-proxy就会通过service的Informer感知到这样一个service对象的天剑，作为对该事件的响应，他会在宿主机（master节点上）添加一条iptable规则：

```shell

ginkgo :: ~ % sudo iptables-save|grep "cluster"
-A KUBE-SERVICES ! -s 10.95.0.0/16 -m comment --comment "Kubernetes service cluster ip + port for masquerade purpose" -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ

ginkgo :: ~/config/service % k get service
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
hostnames    ClusterIP   10.96.24.254    <none>        80/TCP    22m // demo 中创建的案例
kuard        ClusterIP   10.96.162.118   <none>        80/TCP    10d
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   117d
```

这条iptables规则的含义是，凡是目的地是 10.95.0.0/16 端口为80 的IP包，都应该跳转到另一个名字叫 KUBE-MARK-MASQ 的iptables链进行处理。

2、通过IPVS的方式

工作原理：当我们创建了前面的service之后，kube-proxy首先会在宿主机上创建一个虚拟网卡（kube-ipvs0）

```shell
ginkgo :: ~/config/service % ip addr
3: kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default
    link/ether a6:14:f8:62:48:db brd ff:ff:ff:ff:ff:ff
    inet 10.96.0.10/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.0.1/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.56.241/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.189.93/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.58.4/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.130.108/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.187.72/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.162.118/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.240.191/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.24.254/32 scope global kube-ipvs0 // 这个是当前集群创建的hostnames service 配置的service VIP
       valid_lft forever preferred_lft forever
```

kube-proxy会通过Linux的IPVS模块为这个IP地址设置3台IPVS虚拟主机，并且设置这3台虚拟主机之间使用 轮询模式 来作为该负载策略。 可以通过ipvsadm 查看该设置

```shell
ginkgo :: ~/config/service % sudo ipvsadm -ln
TCP  10.96.24.254:80 rr
  -> 10.95.1.46:9376              Masq    1      0          0
  -> 10.95.2.36:9376              Masq    1      0          0
  -> 10.95.2.37:9376              Masq    1      0          0

```

这3台IPVS虚拟主机的IP地址和端口对应的正式3个被代理的pod，任何发往10.96.24.254:80的请求都会被IPVS模块转发到某个后端pod上。


3、两个模式的对比

1）性能上没有显著提升：相对于iptables，IPVS在内核中的实现其实也是基于Netfilter的NAT模式，所以在转发这一层上，理论上IPVS并没有显著的性能提升。

2）IPVS并不需要再宿主机上为每个pod设置iptables规则，而是把这些规则放入到内核态中，从而极大减少了维护这些规则的代价。

3）但是IPVS的仅仅进行上诉的负载均衡和代理功能，一个完整的service流程正常工作所需要的包过滤，SNAT等操作，还是需要iptables来实现。